{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from pydantic import BaseModel\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_key = \"AIzaSyAHPpqUignpGcTI1ZfmXfcFcxlpKDtDSrQ\"\n",
    "api_key = \"AIzaSyCPfIdMffhoR2nxre5pmCFuYmvEI6G7oyY\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name= \"gemini-1.5-flash\"\n",
    "llm = ChatGoogleGenerativeAI(model=model_name,temperature=0.0, google_api_key=api_key, max_tokens=None)\n",
    "# !pip install langchain-google-genai chromadb pypdf pillow unstructured[local-inference] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'docs\\HDFC'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: docs\\HDFC\\HDFC_Ergo_Energy_Gold\\HDFC_Ergo_Energy_Gold_Brochure.pdf\n",
      "Processed: docs\\HDFC\\HDFC_Ergo_Energy_Gold\\HDFC_Ergo_Energy_Gold_Policy_Wording.pdf\n",
      "Processed: docs\\HDFC\\HDFC_Optima_Restore\\optima-restore-one-pager.pdf\n",
      "Processed: docs\\HDFC\\HDFC_Optima_Restore\\or-brochure-revision.pdf\n",
      "Processed: docs\\HDFC\\HDFC_Optima_Restore\\PolicyWording_optima-restore-revision.pdf\n",
      "\n",
      "--- DOCUMENT PROCESSING SUMMARY ---\n",
      "Total original documents: 172\n",
      "Total processed chunks: 172\n",
      "Total text chunks: 92\n",
      "Total table chunks: 80\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def extract_tables_and_text(pdf_path):\n",
    "    documents = []\n",
    "    \n",
    "    try:\n",
    "        # Open the PDF\n",
    "        doc = fitz.open(pdf_path)\n",
    "        \n",
    "        for page_num, page in enumerate(doc):\n",
    "            # Extract text\n",
    "            text_content = page.get_text()\n",
    "            \n",
    "            # Extract tables\n",
    "            tables = page.find_tables()\n",
    "            \n",
    "            # Add text as a document\n",
    "            if text_content.strip():\n",
    "                documents.append(Document(\n",
    "                    page_content=text_content,\n",
    "                    metadata={\n",
    "                        'source': pdf_path,\n",
    "                        'page': page_num,\n",
    "                        'type': 'text'\n",
    "                    }\n",
    "                ))\n",
    "            \n",
    "            # Add tables as separate documents\n",
    "            for table_index, table in enumerate(tables):\n",
    "                # Convert table to markdown\n",
    "                table_markdown = table.to_markdown()\n",
    "                \n",
    "                if table_markdown.strip():\n",
    "                    documents.append(Document(\n",
    "                        page_content=table_markdown,\n",
    "                        metadata={\n",
    "                            'source': pdf_path,\n",
    "                            'page': page_num,\n",
    "                            'table_index': table_index,\n",
    "                            'type': 'table'\n",
    "                        }\n",
    "                    ))\n",
    "        \n",
    "        doc.close()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path}: {e}\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "def load_pdfs_from_directory(directory_path):\n",
    "    all_docs = []\n",
    "    \n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                filepath = os.path.join(root, file)\n",
    "                \n",
    "                # Extract documents including tables\n",
    "                docs = extract_tables_and_text(filepath)\n",
    "                all_docs.extend(docs)\n",
    "                \n",
    "                print(f\"Processed: {filepath}\")\n",
    "    \n",
    "    return all_docs\n",
    "\n",
    "def split_documents(documents):\n",
    "    final_chunks = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        if doc.metadata['type'] == 'table':\n",
    "            # Keep entire table as a chunk\n",
    "            final_chunks.append(Document(\n",
    "                page_content=doc.page_content,\n",
    "                metadata={\n",
    "                    **doc.metadata,\n",
    "                    'content_type': 'table'\n",
    "                }\n",
    "            ))\n",
    "        elif doc.metadata['type'] == 'text':\n",
    "            # Keep text as is without splitting\n",
    "            final_chunks.append(Document(\n",
    "                page_content=doc.page_content,\n",
    "                metadata={\n",
    "                    **doc.metadata,\n",
    "                    'content_type': 'text'\n",
    "                }\n",
    "            ))\n",
    "    return final_chunks\n",
    "\n",
    "def main(path):\n",
    "    documents = load_pdfs_from_directory(path)\n",
    "    processed_documents = split_documents(documents)\n",
    "    \n",
    "    print(\"\\n--- DOCUMENT PROCESSING SUMMARY ---\")\n",
    "    print(f\"Total original documents: {len(documents)}\")\n",
    "    print(f\"Total processed chunks: {len(processed_documents)}\")\n",
    "    \n",
    "    # Count and print types of chunks\n",
    "    text_chunks = [doc for doc in processed_documents if doc.metadata.get('content_type') == 'text']\n",
    "    table_chunks = [doc for doc in processed_documents if doc.metadata.get('content_type') == 'table']\n",
    "    \n",
    "    print(f\"Total text chunks: {len(text_chunks)}\")\n",
    "    print(f\"Total table chunks: {len(table_chunks)}\")\n",
    "    \n",
    "    return documents, processed_documents, text_chunks, table_chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = PATH  # Replace with your directory path\n",
    "    documents, processed_documents, text_chunks, table_chunks = main(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)   \n",
    "len(text_chunks)   \n",
    "len(processed_documents)   \n",
    "# len(table_chunks)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarisation without images, headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import tiktoken\n",
    "\n",
    "def analyze_documents(documents, processed_documents):\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    # Metrics dictionary to store results\n",
    "    metrics = {\n",
    "        'per_pdf': {},\n",
    "        'total': {\n",
    "            'word_count': 0,\n",
    "            'token_count': 0,\n",
    "            'paragraphs': 0,\n",
    "            'page_count': 0,\n",
    "            'tables': 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Track processed PDFs to avoid duplicates\n",
    "    processed_pdfs = set()\n",
    "    \n",
    "    for doc in documents:\n",
    "        source = doc.metadata['source']\n",
    "        \n",
    "        # Skip if this PDF has already been processed\n",
    "        if source in processed_pdfs:\n",
    "            continue\n",
    "        processed_pdfs.add(source)\n",
    "        \n",
    "        # Open PDF to get page count\n",
    "        pdf_doc = fitz.open(source)\n",
    "        page_count = len(pdf_doc)\n",
    "        pdf_doc.close()\n",
    "        \n",
    "        # Filter documents for this specific PDF\n",
    "        pdf_docs = [d for d in documents if d.metadata['source'] == source]\n",
    "        pdf_processed_docs = [d for d in processed_documents if d.metadata['source'] == source]\n",
    "        \n",
    "        # Calculate metrics for this PDF\n",
    "        pdf_metrics = {\n",
    "            'word_count': 0,\n",
    "            'token_count': 0,\n",
    "            'paragraphs': 0,\n",
    "            'page_count': page_count,\n",
    "            'tables': 0\n",
    "        }\n",
    "        \n",
    "        for doc in pdf_docs:\n",
    "            # Count tables\n",
    "            if doc.metadata.get('type') == 'table':\n",
    "                pdf_metrics['tables'] += 1\n",
    "                metrics['total']['tables'] += 1\n",
    "            \n",
    "            # Count words and tokens for text\n",
    "            if doc.page_content:\n",
    "                words = doc.page_content.split()\n",
    "                tokens = tokenizer.encode(doc.page_content)\n",
    "                paragraphs = doc.page_content.split('\\n\\n')\n",
    "                \n",
    "                pdf_metrics['word_count'] += len(words)\n",
    "                pdf_metrics['token_count'] += len(tokens)\n",
    "                pdf_metrics['paragraphs'] += len(paragraphs)\n",
    "        \n",
    "        # Update total metrics\n",
    "        metrics['total']['word_count'] += pdf_metrics['word_count']\n",
    "        metrics['total']['token_count'] += pdf_metrics['token_count']\n",
    "        metrics['total']['paragraphs'] += pdf_metrics['paragraphs']\n",
    "        metrics['total']['page_count'] += page_count\n",
    "        \n",
    "        # Store metrics for this PDF\n",
    "        metrics['per_pdf'][source] = pdf_metrics\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_document_analysis(metrics):\n",
    "    print(\"\\n--- DOCUMENT ANALYSIS ---\")\n",
    "    \n",
    "    # Print per PDF metrics\n",
    "    print(\"\\nPer PDF Metrics:\")\n",
    "    for pdf, pdf_metrics in metrics['per_pdf'].items():\n",
    "        print(f\"\\nPDF: {pdf}\")\n",
    "        print(f\"  Word Count: {pdf_metrics['word_count']}\")\n",
    "        print(f\"  Token Count: {pdf_metrics['token_count']}\")\n",
    "        print(f\"  Paragraphs: {pdf_metrics['paragraphs']}\")\n",
    "        print(f\"  Page Count: {pdf_metrics['page_count']}\")\n",
    "        print(f\"  Tables: {pdf_metrics['tables']}\")\n",
    "    \n",
    "    # Print total metrics\n",
    "    print(\"\\nTotal Metrics:\")\n",
    "    total = metrics['total']\n",
    "    print(f\"  Total Word Count: {total['word_count']}\")\n",
    "    print(f\"  Total Token Count: {total['token_count']}\")\n",
    "    print(f\"  Total Paragraphs: {total['paragraphs']}\")\n",
    "    print(f\"  Total Page Count: {total['page_count']}\")\n",
    "    print(f\"  Total Tables: {total['tables']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DOCUMENT ANALYSIS ---\n",
      "\n",
      "Per PDF Metrics:\n",
      "\n",
      "PDF: docs\\HDFC\\HDFC_Ergo_Energy_Gold\\HDFC_Ergo_Energy_Gold_Brochure.pdf\n",
      "  Word Count: 2120\n",
      "  Token Count: 5987\n",
      "  Paragraphs: 20\n",
      "  Page Count: 12\n",
      "  Tables: 4\n",
      "\n",
      "PDF: docs\\HDFC\\HDFC_Ergo_Energy_Gold\\HDFC_Ergo_Energy_Gold_Policy_Wording.pdf\n",
      "  Word Count: 18545\n",
      "  Token Count: 34661\n",
      "  Paragraphs: 105\n",
      "  Page Count: 39\n",
      "  Tables: 33\n",
      "\n",
      "PDF: docs\\HDFC\\HDFC_Optima_Restore\\optima-restore-one-pager.pdf\n",
      "  Word Count: 510\n",
      "  Token Count: 792\n",
      "  Paragraphs: 2\n",
      "  Page Count: 2\n",
      "  Tables: 0\n",
      "\n",
      "PDF: docs\\HDFC\\HDFC_Optima_Restore\\or-brochure-revision.pdf\n",
      "  Word Count: 2058\n",
      "  Token Count: 3289\n",
      "  Paragraphs: 12\n",
      "  Page Count: 8\n",
      "  Tables: 2\n",
      "\n",
      "PDF: docs\\HDFC\\HDFC_Optima_Restore\\PolicyWording_optima-restore-revision.pdf\n",
      "  Word Count: 25556\n",
      "  Token Count: 46617\n",
      "  Paragraphs: 113\n",
      "  Page Count: 31\n",
      "  Tables: 41\n",
      "\n",
      "Total Metrics:\n",
      "  Total Word Count: 48789\n",
      "  Total Token Count: 91346\n",
      "  Total Paragraphs: 252\n",
      "  Total Page Count: 92\n",
      "  Total Tables: 80\n"
     ]
    }
   ],
   "source": [
    "print_document_analysis(analyze_documents(documents,processed_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with headings & Subheadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DOCUMENT ANALYSIS ---\n",
      "\n",
      "Per PDF Metrics:\n",
      "\n",
      "PDF: docs\\HDFC\\HDFC_Ergo_Energy_Gold\\HDFC_Ergo_Energy_Gold_Brochure.pdf\n",
      "  Word Count: 2120\n",
      "  Token Count: 5987\n",
      "  Paragraphs: 20\n",
      "  Page Count: 12\n",
      "  Tables: 4\n",
      "  Headings:\n",
      "    Total Headings: 1\n",
      "    Headings by Level:\n",
      "      Section: 1\n",
      "\n",
      "PDF: docs\\HDFC\\HDFC_Ergo_Energy_Gold\\HDFC_Ergo_Energy_Gold_Policy_Wording.pdf\n",
      "  Word Count: 18545\n",
      "  Token Count: 34661\n",
      "  Paragraphs: 105\n",
      "  Page Count: 39\n",
      "  Tables: 33\n",
      "  Headings:\n",
      "    Total Headings: 26\n",
      "    Headings by Level:\n",
      "      Section: 26\n",
      "\n",
      "PDF: docs\\HDFC\\HDFC_Optima_Restore\\optima-restore-one-pager.pdf\n",
      "  Word Count: 510\n",
      "  Token Count: 792\n",
      "  Paragraphs: 2\n",
      "  Page Count: 2\n",
      "  Tables: 0\n",
      "  Headings:\n",
      "    Total Headings: 0\n",
      "    Headings by Level:\n",
      "\n",
      "PDF: docs\\HDFC\\HDFC_Optima_Restore\\or-brochure-revision.pdf\n",
      "  Word Count: 2058\n",
      "  Token Count: 3289\n",
      "  Paragraphs: 12\n",
      "  Page Count: 8\n",
      "  Tables: 2\n",
      "  Headings:\n",
      "    Total Headings: 0\n",
      "    Headings by Level:\n",
      "\n",
      "PDF: docs\\HDFC\\HDFC_Optima_Restore\\PolicyWording_optima-restore-revision.pdf\n",
      "  Word Count: 25556\n",
      "  Token Count: 46617\n",
      "  Paragraphs: 113\n",
      "  Page Count: 31\n",
      "  Tables: 41\n",
      "  Headings:\n",
      "    Total Headings: 32\n",
      "    Headings by Level:\n",
      "      Section: 32\n",
      "\n",
      "Total Metrics:\n",
      "  Total Word Count: 48789\n",
      "  Total Token Count: 91346\n",
      "  Total Paragraphs: 252\n",
      "  Total Page Count: 92\n",
      "  Total Tables: 80\n",
      "  Headings:\n",
      "    Total Headings: 59\n",
      "    Headings by Level:\n",
      "      Section: 59\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import tiktoken\n",
    "import os\n",
    "import re\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def extract_headings(text):\n",
    "    # Regular expressions to capture different heading levels\n",
    "    heading_patterns = [\n",
    "        (r'^#\\s+(.+)$', 'H1'),     # Markdown-style H1\n",
    "        (r'^##\\s+(.+)$', 'H2'),    # Markdown-style H2\n",
    "        (r'^###\\s+(.+)$', 'H3'),   # Markdown-style H3\n",
    "        (r'^\\s*([A-Z][A-Za-z\\s]+):$', 'Section'),  # Section-style headings\n",
    "    ]\n",
    "    \n",
    "    headings = []\n",
    "    for pattern, level in heading_patterns:\n",
    "        matches = re.findall(pattern, text, re.MULTILINE)\n",
    "        for match in matches:\n",
    "            headings.append({\n",
    "                'text': match.strip(),\n",
    "                'level': level\n",
    "            })\n",
    "    \n",
    "    return headings\n",
    "\n",
    "def analyze_documents(documents, processed_documents):\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    # Metrics dictionary to store results\n",
    "    metrics = {\n",
    "        'per_pdf': {},\n",
    "        'total': {\n",
    "            'word_count': 0,\n",
    "            'token_count': 0,\n",
    "            'paragraphs': 0,\n",
    "            'page_count': 0,\n",
    "            'tables': 0,\n",
    "            'headings': {\n",
    "                'total': 0,\n",
    "                'by_level': {},\n",
    "                'names': []\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Track processed PDFs to avoid duplicates\n",
    "    processed_pdfs = set()\n",
    "    \n",
    "    for doc in documents:\n",
    "        source = doc.metadata['source']\n",
    "        \n",
    "        # Skip if this PDF has already been processed\n",
    "        if source in processed_pdfs:\n",
    "            continue\n",
    "        processed_pdfs.add(source)\n",
    "        \n",
    "        # Open PDF to get page count\n",
    "        pdf_doc = fitz.open(source)\n",
    "        page_count = len(pdf_doc)\n",
    "        pdf_doc.close()\n",
    "        \n",
    "        # Filter documents for this specific PDF\n",
    "        pdf_docs = [d for d in documents if d.metadata['source'] == source]\n",
    "        \n",
    "        # Calculate metrics for this PDF\n",
    "        pdf_metrics = {\n",
    "            'word_count': 0,\n",
    "            'token_count': 0,\n",
    "            'paragraphs': 0,\n",
    "            'page_count': page_count,\n",
    "            'tables': 0,\n",
    "            'headings': {\n",
    "                'total': 0,\n",
    "                'by_level': {},\n",
    "                'names': []\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for doc in pdf_docs:\n",
    "            # Count tables\n",
    "            if doc.metadata.get('type') == 'table':\n",
    "                pdf_metrics['tables'] += 1\n",
    "                metrics['total']['tables'] += 1\n",
    "            \n",
    "            # Count words and tokens for text\n",
    "            if doc.page_content:\n",
    "                words = doc.page_content.split()\n",
    "                tokens = tokenizer.encode(doc.page_content)\n",
    "                paragraphs = doc.page_content.split('\\n\\n')\n",
    "                \n",
    "                # Extract headings\n",
    "                headings = extract_headings(doc.page_content)\n",
    "                \n",
    "                pdf_metrics['word_count'] += len(words)\n",
    "                pdf_metrics['token_count'] += len(tokens)\n",
    "                pdf_metrics['paragraphs'] += len(paragraphs)\n",
    "                \n",
    "                # Process headings\n",
    "                pdf_metrics['headings']['total'] += len(headings)\n",
    "                metrics['total']['headings']['total'] += len(headings)\n",
    "                \n",
    "                for heading in headings:\n",
    "                    # Track heading by level\n",
    "                    level = heading['level']\n",
    "                    pdf_metrics['headings']['by_level'][level] = pdf_metrics['headings']['by_level'].get(level, 0) + 1\n",
    "                    metrics['total']['headings']['by_level'][level] = metrics['total']['headings']['by_level'].get(level, 0) + 1\n",
    "                    \n",
    "                    # Store heading names\n",
    "                    pdf_metrics['headings']['names'].append(heading['text'])\n",
    "                    metrics['total']['headings']['names'].append(heading['text'])\n",
    "        \n",
    "        # Update total metrics\n",
    "        metrics['total']['word_count'] += pdf_metrics['word_count']\n",
    "        metrics['total']['token_count'] += pdf_metrics['token_count']\n",
    "        metrics['total']['paragraphs'] += pdf_metrics['paragraphs']\n",
    "        metrics['total']['page_count'] += page_count\n",
    "        \n",
    "        # Store metrics for this PDF\n",
    "        metrics['per_pdf'][source] = pdf_metrics\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_document_analysis(metrics):\n",
    "    print(\"\\n--- DOCUMENT ANALYSIS ---\")\n",
    "    \n",
    "    # Print per PDF metrics\n",
    "    print(\"\\nPer PDF Metrics:\")\n",
    "    for pdf, pdf_metrics in metrics['per_pdf'].items():\n",
    "        print(f\"\\nPDF: {pdf}\")\n",
    "        print(f\"  Word Count: {pdf_metrics['word_count']}\")\n",
    "        print(f\"  Token Count: {pdf_metrics['token_count']}\")\n",
    "        print(f\"  Paragraphs: {pdf_metrics['paragraphs']}\")\n",
    "        print(f\"  Page Count: {pdf_metrics['page_count']}\")\n",
    "        print(f\"  Tables: {pdf_metrics['tables']}\")\n",
    "        \n",
    "        # Headings metrics\n",
    "        print(\"  Headings:\")\n",
    "        print(f\"    Total Headings: {pdf_metrics['headings']['total']}\")\n",
    "        \n",
    "        # Print headings by level\n",
    "        print(\"    Headings by Level:\")\n",
    "        for level, count in pdf_metrics['headings']['by_level'].items():\n",
    "            print(f\"      {level}: {count}\")\n",
    "        \n",
    "        # # Print first 10 heading names\n",
    "        # print(\"    Heading Names (first 10):\")\n",
    "        # for heading in pdf_metrics['headings']['names'][:10]:\n",
    "        #     print(f\"      - {heading}\")\n",
    "        # if len(pdf_metrics['headings']['names']) > 10:\n",
    "        #     print(f\"      ... and {len(pdf_metrics['headings']['names']) - 10} more\")\n",
    "    \n",
    "    # Print total metrics\n",
    "    print(\"\\nTotal Metrics:\")\n",
    "    total = metrics['total']\n",
    "    print(f\"  Total Word Count: {total['word_count']}\")\n",
    "    print(f\"  Total Token Count: {total['token_count']}\")\n",
    "    print(f\"  Total Paragraphs: {total['paragraphs']}\")\n",
    "    print(f\"  Total Page Count: {total['page_count']}\")\n",
    "    print(f\"  Total Tables: {total['tables']}\")\n",
    "    \n",
    "    # Total Headings metrics\n",
    "    print(\"  Headings:\")\n",
    "    print(f\"    Total Headings: {total['headings']['total']}\")\n",
    "    \n",
    "    # Print total headings by level\n",
    "    print(\"    Headings by Level:\")\n",
    "    for level, count in total['headings']['by_level'].items():\n",
    "        print(f\"      {level}: {count}\")\n",
    "    \n",
    "    # # Print first 20 total heading names\n",
    "    # print(\"    Heading Names (first 20):\")\n",
    "    # for heading in total['headings']['names'][:20]:\n",
    "    #     print(f\"      - {heading}\")\n",
    "    # if len(total['headings']['names']) > 20:\n",
    "    #     print(f\"      ... and {len(total['headings']['names']) - 20} more\")\n",
    "\n",
    "\n",
    "\n",
    "metrics = analyze_documents(documents, processed_documents)\n",
    "print_document_analysis(metrics)\n",
    "    # Print analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(processed_documents)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = 'docs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: docs\\Aditya Birla\\AdityaBirla_Activ Health Platinum Essential\\ABHI_Active_health_prospectus (1).pdf\n",
      "Processed: docs\\Aditya Birla\\AdityaBirla_Activ Health Platinum Essential\\Activ_health_addons.pdf\n",
      "Processed: docs\\Aditya Birla\\AdityaBirla_Activ Health Platinum Essential\\Activ_health_pW.pdf\n",
      "Processed: docs\\Aditya Birla\\AdityaBirla_Activ Health Platinum Essential\\OPD_BROCHURE.pdf\n",
      "Processed: docs\\Aditya Birla\\AdityaBirla_Activ Health Platinum Essential\\Platinum_Essential_brochure.pdf\n",
      "Processed: docs\\Aditya Birla\\AdityaBirla_Activ_Fit\\AdityaBirla_Activ Fit_Product_Benefits_Table.pdf\n",
      "Processed: docs\\Aditya Birla\\AdityaBirla_Activ_Fit\\AdityaBirla_Activ_Fit_Brochure.pdf\n",
      "Processed: docs\\Aditya Birla\\AdityaBirla_Activ_Fit\\AdityaBirla_Activ_Fit_Policy_Wording.pdf\n",
      "Processed: docs\\Aditya Birla\\AdityaBirla_Activ_Health_Platinum_Enhanced\\ABHI_Active_health_prospectus.pdf\n",
      "Processed: docs\\Aditya Birla\\AdityaBirla_Activ_Health_Platinum_Enhanced\\Activ_health_addons.pdf\n",
      "Processed: docs\\Aditya Birla\\AdityaBirla_Activ_Health_Platinum_Enhanced\\Activ_health_Brochure.pdf\n",
      "Processed: docs\\Aditya Birla\\AdityaBirla_Activ_Health_Platinum_Enhanced\\Activ_health_pW.pdf\n",
      "Processed: docs\\Aditya Birla\\AdityaBirla_Activ_Health_Platinum_Enhanced\\OPD_BROCHURE.pdf\n",
      "Processed: docs\\Aditya Birla\\AdityaBirla_activ_one\\AdityaBirla_Activ one policy wording.pdf\n",
      "Processed: docs\\Aditya Birla\\AdityaBirla_activ_one\\AdityaBirla_activ_one_brochure.pdf\n",
      "Processed: docs\\Aditya Birla\\AdityaBirla_activ_one\\AdityaBirla_activ_one_Prospectus_.pdf\n",
      "Processed: docs\\BajajAllianz\\Bajaj_Allianz_Network_List.pdf\n",
      "Processed: docs\\BajajAllianz\\Bajaj_My Health Care\\My Health Care plan_brochure_Plan_5_R1.pdf\n",
      "Processed: docs\\BajajAllianz\\Bajaj_My Health Care\\MyHealthCare_PW.pdf\n",
      "Processed: docs\\Care\\care-senior-brochure.pdf\n",
      "Processed: docs\\Care\\Care_Network_List.pdf\n",
      "Processed: docs\\Care\\CARE Heart\\Care Heart Prospectus.pdf\n",
      "Processed: docs\\Care\\CARE Heart\\care-heart---piano-fold-brochure---web.pdf\n",
      "Processed: docs\\Care\\CARE Heart\\care-heart---policy-terms-&-conditions-(effective-from-01-april-2021).pdf\n",
      "Processed: docs\\Care\\Care Joy\\Care_joy-(maternity-insurance-product)--prospectus-cum-sales-literature.pdf\n",
      "Processed: docs\\Care\\Care Joy\\Care_Joy_Brochure.pdf\n",
      "Processed: docs\\Care\\Care Joy\\Care_Joy_Proposal_form.pdf\n",
      "Processed: docs\\Care\\Care Joy\\Care_Joy_TandC.pdf\n",
      "Processed: docs\\Care\\Care Senior Health Advantage\\senior-health-advantage_brochure.pdf\n",
      "Processed: docs\\Care\\Care Senior Health Advantage\\senior-health-advantage_policy-terms-and-conditions.pdf\n",
      "Processed: docs\\Care\\CARE SUPREME\\CARE SUPREME - BROCHURE.pdf\n",
      "Processed: docs\\Care\\CARE SUPREME\\CARE SUPREME PROSPECTUS.pdf\n",
      "Processed: docs\\Care\\CARE SUPREME\\care-supreme---policy-terms-and-conditions.pdf\n",
      "Processed: docs\\Care\\Care Supreme Combo\\CARE SUPREME - BROCHURE.pdf\n",
      "Processed: docs\\Care\\Care Supreme Combo\\CARE SUPREME PROSPECTUS.pdf\n",
      "Processed: docs\\Care\\Care Supreme Combo\\care-supreme---policy-terms-and-conditions.pdf\n",
      "Processed: docs\\Care\\Care Supreme Combo\\supreme-enhance---brochure.pdf\n",
      "Processed: docs\\Care\\Care Supreme Combo\\supreme-enhance---prospectus-cum-sales-literature.pdf\n",
      "Processed: docs\\Care\\Care Supreme Senior Premium\\CARE SUPREME - BROCHURE_TA_LR.pdf\n",
      "Processed: docs\\Care\\Care Supreme Senior Premium\\CARE SUPREME PROSPECTUS.pdf\n",
      "Processed: docs\\Care\\Care Supreme Senior Premium\\care-supreme---policy-terms-and-conditions.pdf\n",
      "Processed: docs\\Care\\CARE-PLUS\\care-plus--(health-insurance-product)---policy-t&c (1).pdf\n",
      "Processed: docs\\Care\\CARE-PLUS\\CARE-PLUS-COMPLETE-HEALTH-INSURANCE-PLAN-BROCHURE.pdf\n",
      "Processed: docs\\Care\\Care_Advantage\\care-advantage-(health-insurance-product)--prospectus-cum-sales-literature.pdf\n",
      "Processed: docs\\Care\\Care_Advantage\\care-advantage-(health-insurance-product)-brochure.pdf\n",
      "Processed: docs\\Care\\Care_Advantage\\care-advantage-policywording.pdf\n",
      "Processed: docs\\Care\\Care_Advantage\\Care_Advantage_Brochure.pdf\n",
      "Processed: docs\\Care\\Care_Advantage\\Care_Advantage_claim_form.pdf\n",
      "Processed: docs\\Care\\Care_Advantage\\Care_Advantage_Proposal_form.pdf\n",
      "Processed: docs\\Care\\Care_Freedom\\care-freedom-(health-insurance-product)---prospectus-cum-sales-literature (1).pdf\n",
      "Processed: docs\\Care\\Care_Freedom\\care-freedom_prospectus-cum-sales-literature.pdf\n",
      "Processed: docs\\Care\\Care_Freedom\\Care_Freedom_Brochure.pdf\n",
      "Processed: docs\\Care\\Care_Freedom\\Care_Freedom_Claim_form.pdf\n",
      "Processed: docs\\Care\\Care_Freedom\\Care_Freedom_Proposal_form.pdf\n",
      "Processed: docs\\Care\\Care_Freedom\\Care_Freedom_TandC.pdf\n",
      "Processed: docs\\Care\\Care_Ultimate_Care\\ultimate-care-----policy-terms-&-conditions.pdf\n",
      "Processed: docs\\Galaxy\\Galaxy_Promise - Elite\\Galaxy Promise - Brochure.pdf\n",
      "Processed: docs\\Galaxy\\Galaxy_Promise - Elite\\Galaxy Promise - Policy wordings.pdf\n",
      "Processed: docs\\HDFC\\HDFC-optima_secure\\HDFC-optima-secure-brochure.pdf\n",
      "Processed: docs\\HDFC\\HDFC-optima_secure\\HDFC-optima-secure-prospectus.pdf\n",
      "Processed: docs\\HDFC\\HDFC-optima_secure\\HDFC-optima_secure_policywordings.pdf\n",
      "Processed: docs\\HDFC\\HDFC_Ergo_Energy_Gold\\HDFC_Ergo_Energy_Gold_Brochure.pdf\n",
      "Processed: docs\\HDFC\\HDFC_Ergo_Energy_Gold\\HDFC_Ergo_Energy_Gold_Policy_Wording.pdf\n",
      "Processed: docs\\HDFC\\HDFC_Optima_Restore\\optima-restore-one-pager.pdf\n",
      "Processed: docs\\HDFC\\HDFC_Optima_Restore\\or-brochure-revision.pdf\n",
      "Processed: docs\\HDFC\\HDFC_Optima_Restore\\PolicyWording_optima-restore-revision.pdf\n",
      "Processed: docs\\ICICI Lombard\\ICICI Elevate\\ICICI_Elevate_Brochure.pdf\n",
      "Processed: docs\\ICICI Lombard\\ICICI Elevate\\ICICI_Elevate_Policy_Wordings.pdf\n",
      "Processed: docs\\ICICI Lombard\\ICICI_Health_AdvantEdge\\HA_Prospectus_website1.pdf\n",
      "Processed: docs\\ICICI Lombard\\ICICI_Health_AdvantEdge\\Health AdvantEdge_Policy wordings_IL.pdf\n",
      "Processed: docs\\ICICI Lombard\\ICICI_Health_AdvantEdge\\health-advantedge-apex-plus-plan-brochure.pdf\n",
      "Processed: docs\\ICICI Lombard\\ICICI_MaxProtect\\ICICI_maxprotect-policy-wordings.pdf\n",
      "Processed: docs\\ICICI Lombard\\ICICI_MaxProtect\\ICICI_MaxProtect_brochure.pdf\n",
      "Processed: docs\\ICICI Lombard\\ICICI_Max_Protect_Classic\\maxprotect-brochure.pdf\n",
      "Processed: docs\\ICICI Lombard\\ICICI_Max_Protect_Classic\\policy-wordings_maxprotect.pdf\n",
      "Processed: docs\\ICICI Lombard\\ICICI_Max_Protect_Classic\\prospectus-and-sales-literature_maxprotect.pdf\n",
      "Processed: docs\\ICICI Lombard\\ICICI_Supertopup_healthbooster\\health-booster_policy-wordings.pdf\n",
      "Processed: docs\\ICICI Lombard\\ICICI_Supertopup_healthbooster\\health_booster_brochure.pdf\n",
      "Processed: docs\\Niva Bupa\\NivaBupa Health ReAssure\\Acute Care Brochure.pdf\n",
      "Processed: docs\\Niva Bupa\\NivaBupa Health ReAssure\\Disease Management Brochure.pdf\n",
      "Processed: docs\\Niva Bupa\\NivaBupa Health ReAssure\\Health_ReAssurePolicyDocument.pdf\n",
      "Processed: docs\\Niva Bupa\\NivaBupa Rise\\Ab India Karega Rise_Single Sheeter for Policy Bazaar_v7 revised.pdf\n",
      "Processed: docs\\Niva Bupa\\NivaBupa Rise\\Rise_Policy Wordings_Website.pdf\n",
      "Processed: docs\\Niva Bupa\\NivaBupa_Arogya_Sanjeevani\\Arogya_Sanjeevani_PolicyDocument_draft v17.pdf\n",
      "Processed: docs\\Niva Bupa\\NivaBupa_Arogya_Sanjeevani\\Niva_PB_Arogya Sanjeevani_SS_v6.pdf\n",
      "Processed: docs\\Niva Bupa\\NivaBupa_Aspire\\NivaBupa_Aspire_Brochure.pdf\n",
      "Processed: docs\\Niva Bupa\\NivaBupa_Aspire\\NivaBupa_Aspire_policy_wordings.pdf\n",
      "Processed: docs\\Niva Bupa\\NivaBupa_Aspire\\NivaBupa_Aspire_Prospectus.pdf\n",
      "Processed: docs\\Niva Bupa\\NivaBupa_Aspire_Gold+ Value (Direct)\\Aspire_Brochure.pdf\n",
      "Processed: docs\\Niva Bupa\\NivaBupa_Aspire_Gold+ Value (Direct)\\Aspire_Prospectus.pdf\n",
      "Processed: docs\\Niva Bupa\\NivaBupa_Aspire_Gold+ Value (Direct)\\Aspire_pw.pdf\n",
      "Processed: docs\\Niva Bupa\\NivaBupa_Go_Active\\GoActive_PolicyWording.pdf\n",
      "Processed: docs\\Niva Bupa\\NivaBupa_Go_Active\\Niva_PB_Go Active_SS_v6.pdf\n",
      "Processed: docs\\Niva Bupa\\NivaBupa_Health_Companion\\Health Companion V2022 - Brochure.pdf\n",
      "Processed: docs\\Niva Bupa\\NivaBupa_Health_Companion\\Health Companion V2022 - One Pager.pdf\n",
      "Processed: docs\\Niva Bupa\\NivaBupa_Health_Companion\\Health Companion_V5_Policy Wording.pdf\n",
      "Processed: docs\\Niva Bupa\\NivaBupa_Health_Premia\\Health Premia Claim Form.pdf\n",
      "Processed: docs\\Niva Bupa\\NivaBupa_Health_Premia\\Health Premia Policy Wording.pdf\n",
      "Processed: docs\\Niva Bupa\\NivaBupa_Health_Premia\\Health Premia Proposal Form.pdf\n",
      "Processed: docs\\Niva Bupa\\NivaBupa_Health_Premia\\Health Premia Prospectus.pdf\n",
      "Processed: docs\\Niva Bupa\\NivaBupa_Health_Premia\\Niva_Health Premia - Brochure.pdf\n",
      "Processed: docs\\Niva Bupa\\NivaBupa_Health_Pulse_Enhanced\\HealthPulse_Policy Wording.pdf\n",
      "Processed: docs\\Niva Bupa\\NivaBupa_Health_Pulse_Enhanced\\Niva_PB_Health Pulse_SS_v5.pdf\n",
      "Processed: docs\\Niva Bupa\\NivaBupa_ReAssure\\NivaBupa_ReAssure2.0_bronze_Plus_One_Pager.pdf\n",
      "Processed: docs\\Niva Bupa\\NivaBupa_ReAssure\\NivaBupa_ReAssure2.0_Policy Wording.pdf\n",
      "Processed: docs\\Niva Bupa\\ReAssure 2.0 bronze Plus\\ReAssure 2.0 - Policy Wording.pdf\n",
      "Processed: docs\\Niva Bupa\\ReAssure 2.0 bronze Plus\\ReAssure 2.0 bronze Plus One Pager.pdf\n",
      "Processed: docs\\Star Health\\Star_Health_Network_List.pdf\n",
      "Processed: docs\\Star Health\\Star Health_Super Star\\Star Health_Super Star_Brochure.pdf\n",
      "Processed: docs\\Star Health\\Star Health_Super Star\\Star Health_Super Star_policy_wordings.pdf\n",
      "Processed: docs\\Star Health\\Star Health_Super Star\\Star Health_Super Star_Prospectus.pdf\n",
      "Processed: docs\\Star Health\\StarHealthMediClassic\\Brochure - Medi Classic Insurance Policy (Individual) - V.18_Web.pdf\n",
      "Processed: docs\\Star Health\\StarHealthMediClassic\\Home_Care_Treatment_Covid (1).pdf\n",
      "Processed: docs\\Star Health\\StarHealthMediClassic\\Policy - Medi Classic Insurance Policy (Individual) - V.14.pdf\n",
      "Processed: docs\\Star Health\\StarHealthMediClassic\\Proposal Form - Common - V.15.pdf\n",
      "Processed: docs\\Star Health\\StarHealthMediClassic\\Prospectus - Medi Classic Insurance Policy (Individual) - V.13.pdf\n",
      "Processed: docs\\Star Health\\StarHealthMediClassic\\Star Health Claim Form.pdf\n",
      "Processed: docs\\Star Health\\StarHealth_Assure\\StarHealthAssureInsurancePolicy-Brochure.pdf\n",
      "Processed: docs\\Star Health\\StarHealth_Assure\\StarHealthAssureInsurancePolicy-Policy.pdf\n",
      "Processed: docs\\Star Health\\StarHealth_Assure\\StarHealthAssureInsurancePolicy-ProposalForm.pdf\n",
      "Processed: docs\\Star Health\\StarHealth_Assure\\StarHealthAssureInsurancePolicy-Prospectus.pdf\n",
      "Processed: docs\\Star Health\\StarHealth_Cardiac-Care\\StarHealth-Cardiac-Care-Brochure-Platinum.pdf\n",
      "Processed: docs\\Star Health\\StarHealth_Comprehensive\\Combined Proposal Form.pdf\n",
      "Processed: docs\\Star Health\\StarHealth_Comprehensive\\Home_Care_Treatment_Covid.pdf\n",
      "Processed: docs\\Star Health\\StarHealth_Comprehensive\\One pager - ENGLISH.pdf\n",
      "Processed: docs\\Star Health\\StarHealth_Comprehensive\\One pager.pdf\n",
      "Processed: docs\\Star Health\\StarHealth_Comprehensive\\Policy - Star Comprehensive Insurance Policy - V.22 - Final.pdf\n",
      "Processed: docs\\Star Health\\StarHealth_Comprehensive\\Prospectus - Star Comprehensive Insurance Policy - V.13.pdf\n",
      "Processed: docs\\Star Health\\StarHealth_Comprehensive\\Star Health Claim Form.pdf\n",
      "Processed: docs\\Star Health\\StarHealth_Comprehensive\\Star-Comprehensive-brochure-new-1.pdf\n",
      "Processed: docs\\Star Health\\StarHealth_Smart_Health_Pro\\Brochure - Smart Health Pro - V.1_Web (1) (1).pdf\n",
      "Processed: docs\\Star Health\\StarHealth_Smart_Health_Pro\\Policy - Smart Health Pro - V.1_Web.pdf\n",
      "Processed: docs\\Star Health\\StarHealth_Smart_Health_Pro\\SmartHealthProOnePager.pdf\n",
      "Processed: docs\\Star Health\\Star_Health_Young Star Gold Plan\\Brochure-Young-Star-Insurance-Policy.pdf\n",
      "Processed: docs\\Star Health\\Star_Health_Young Star Gold Plan\\Customer-Info-Sheet.pdf\n",
      "Processed: docs\\Star Health\\Star_Health_Young Star Gold Plan\\Home_Care_Treatment_Covid.pdf\n",
      "Processed: docs\\Star Health\\Star_Health_Young Star Gold Plan\\One pager - ENGLISH.pdf\n",
      "Processed: docs\\Star Health\\Star_Health_Young Star Gold Plan\\One pager.pdf\n",
      "Processed: docs\\Star Health\\Star_Health_Young Star Gold Plan\\PolicyClause-Young-Star-Insurance-Policy.pdf\n",
      "Processed: docs\\Star Health\\Star_Health_Young Star Gold Plan\\ProposalForm-Young-Star-Insurance-Policy.pdf\n",
      "Processed: docs\\Star Health\\Star_Health_Young Star Gold Plan\\Prospectus-Young-Star-Insurance-Policy.pdf\n",
      "Processed: docs\\Tata AIG\\Tata_AIG_Network_List.pdf\n",
      "Processed: docs\\Tata AIG\\Tata_Aig_Medicare\\Tata_Aig_Medicare_medicare-prospectus.pdf\n",
      "Processed: docs\\Tata AIG\\Tata_Aig_Medicare\\Tata_Aig_Medicare_Medicare_Brochure.pdf\n",
      "Processed: docs\\Tata AIG\\Tata_Aig_Medicare\\Tata_Aig_Medicare_Policy_Wording.pdf\n",
      "Processed: docs\\Tata AIG\\TATA_AIG_Medicare_Lite\\PolicyWordings_.pdf\n",
      "Processed: docs\\Tata AIG\\TATA_AIG_Medicare_Lite\\ProductBrochure.pdf\n",
      "Processed: docs\\Tata AIG\\TATA_AIG_Medicare_Plus\\Medicare Plus Brochere.pdf\n",
      "Processed: docs\\Tata AIG\\TATA_AIG_Medicare_Plus\\MediCare Plus_onepager.pdf\n",
      "Processed: docs\\Tata AIG\\TATA_AIG_Medicare_Plus\\Meidcare Plus Policy wording.pdf\n",
      "\n",
      "--- DOCUMENT PROCESSING SUMMARY ---\n",
      "Total original documents: 7509\n",
      "Total processed chunks: 7509\n",
      "Total text chunks: 4666\n",
      "Total table chunks: 2843\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "\n",
    "def extract_tables_and_text(pdf_path: str) -> List[Document]:\n",
    "    documents = []\n",
    "\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        \n",
    "        for page_num, page in enumerate(doc):\n",
    "            text_content = page.get_text()\n",
    "            tables = page.find_tables()\n",
    "            \n",
    "            if text_content.strip():\n",
    "                documents.append(Document(\n",
    "                    page_content=text_content,\n",
    "                    metadata={\n",
    "                        'source': pdf_path,\n",
    "                        'page': page_num,\n",
    "                        'type': 'text',\n",
    "                        'subdirectory': os.path.dirname(pdf_path)\n",
    "                    }\n",
    "                ))\n",
    "            \n",
    "            for table_index, table in enumerate(tables):\n",
    "                table_markdown = table.to_markdown()\n",
    "                \n",
    "                if table_markdown.strip():\n",
    "                    documents.append(Document(\n",
    "                        page_content=table_markdown,\n",
    "                        metadata={\n",
    "                            'source': pdf_path,\n",
    "                            'page': page_num,\n",
    "                            'table_index': table_index,\n",
    "                            'type': 'table',\n",
    "                            'subdirectory': os.path.dirname(pdf_path)\n",
    "                        }\n",
    "                    ))\n",
    "        \n",
    "        doc.close()\n",
    "    \n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    return documents\n",
    "def load_pdfs_from_directory(directory_path: str, max_depth: int = 3) -> List[Document]:\n",
    "    all_docs = []\n",
    "\n",
    "    def is_valid_depth(current_path: str) -> bool:\n",
    "        relative_path = os.path.relpath(current_path, directory_path)\n",
    "        depth = relative_path.count(os.sep)\n",
    "        return depth < max_depth\n",
    "\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        if not is_valid_depth(root):\n",
    "            dirs.clear()\n",
    "            continue\n",
    "\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                filepath = os.path.join(root, file)\n",
    "                docs = extract_tables_and_text(filepath)\n",
    "                all_docs.extend(docs)\n",
    "                print(f\"Processed: {filepath}\")\n",
    "\n",
    "    return all_docs\n",
    "\n",
    "def split_documents(documents):\n",
    "    final_chunks = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        if doc.metadata['type'] == 'table':\n",
    "            # Keep entire table as a chunk\n",
    "            final_chunks.append(Document(\n",
    "                page_content=doc.page_content,\n",
    "                metadata={\n",
    "                    **doc.metadata,\n",
    "                    'content_type': 'table'\n",
    "                }\n",
    "            ))\n",
    "        elif doc.metadata['type'] == 'text':\n",
    "            # Keep text as is without splitting\n",
    "            final_chunks.append(Document(\n",
    "                page_content=doc.page_content,\n",
    "                metadata={\n",
    "                    **doc.metadata,\n",
    "                    'content_type': 'text'\n",
    "                }\n",
    "            ))\n",
    "    return final_chunks\n",
    "\n",
    "def main(path):\n",
    "    documents = load_pdfs_from_directory(path)\n",
    "    processed_documents = split_documents(documents)\n",
    "    \n",
    "    print(\"\\n--- DOCUMENT PROCESSING SUMMARY ---\")\n",
    "    print(f\"Total original documents: {len(documents)}\")\n",
    "    print(f\"Total processed chunks: {len(processed_documents)}\")\n",
    "    \n",
    "    # Count and print types of chunks\n",
    "    text_chunks = [doc for doc in processed_documents if doc.metadata.get('content_type') == 'text']\n",
    "    table_chunks = [doc for doc in processed_documents if doc.metadata.get('content_type') == 'table']\n",
    "    \n",
    "    print(f\"Total text chunks: {len(text_chunks)}\")\n",
    "    print(f\"Total table chunks: {len(table_chunks)}\")\n",
    "    \n",
    "    return documents, processed_documents, text_chunks, table_chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = MAIN_PATH  # Replace with your directory path\n",
    "    documents, processed_documents, text_chunks, table_chunks = main(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def save_documents(\n",
    "    documents: List[Document], \n",
    "    processed_documents: List[Document], \n",
    "    text_chunks: List[Document], \n",
    "    table_chunks: List[Document], \n",
    "    output_dir: str = 'extracted_docs'\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    def save_pickle(data: List[Document], filename: str):\n",
    "        with open(os.path.join(output_dir, filename), 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "    def save_json(data: List[Document], filename: str):\n",
    "        serializable_data = [\n",
    "            {\n",
    "                'page_content': doc.page_content, \n",
    "                'metadata': doc.metadata\n",
    "            } for doc in data\n",
    "        ]\n",
    "        with open(os.path.join(output_dir, filename), 'w') as f:\n",
    "            json.dump(serializable_data, f, indent=2)\n",
    "\n",
    "    save_pickle(documents, 'original_documents.pkl')\n",
    "    save_pickle(processed_documents, 'processed_documents.pkl')\n",
    "    save_pickle(text_chunks, 'text_chunks.pkl')\n",
    "    save_pickle(table_chunks, 'table_chunks.pkl')\n",
    "\n",
    "    save_json(documents, 'original_documents.json')\n",
    "    save_json(processed_documents, 'processed_documents.json')\n",
    "    save_json(text_chunks, 'text_chunks.json')\n",
    "    save_json(table_chunks, 'table_chunks.json')\n",
    "\n",
    "    print(f\"Documents saved in {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents saved in extracted_docs\n"
     ]
    }
   ],
   "source": [
    "save_documents(documents, processed_documents, text_chunks, table_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To LOad\n",
    "\n",
    "# import os\n",
    "# import pickle\n",
    "# from typing import List, Tuple\n",
    "# from langchain_core.documents import Document\n",
    "\n",
    "# def load_saved_documents(output_dir: str = 'extracted_docs') -> Tuple[List[Document], List[Document], List[Document], List[Document]]:\n",
    "#     def load_pickle(filename: str) -> List[Document]:\n",
    "#         filepath = os.path.join(output_dir, filename)\n",
    "#         with open(filepath, 'rb') as f:\n",
    "#             return pickle.load(f)\n",
    "\n",
    "#     documents = load_pickle('original_documents.pkl')\n",
    "#     processed_documents = load_pickle('processed_documents.pkl')\n",
    "#     text_chunks = load_pickle('text_chunks.pkl')\n",
    "#     table_chunks = load_pickle('table_chunks.pkl')\n",
    "\n",
    "#     return documents, processed_documents, text_chunks, table_chunks\n",
    "\n",
    "# # Load the saved documents\n",
    "# documents, processed_documents, text_chunks, table_chunks = load_saved_documents()\n",
    "\n",
    "# # Or specify a custom directory\n",
    "# # documents, processed_documents, text_chunks, table_chunks = load_saved_documents('/path/to/your/saved/docs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0                                               1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        2                                                          3\n",
      "S.\n",
      "No.                                           Title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Description                                      Policy clause\n",
      "number\n",
      "      3 What are\n",
      "the major\n",
      "exclusions in\n",
      "the Policy: We will not pay expenses arising from.\n",
      "i. Investigation & Evaluation: Code  Excl04\n",
      "a. Expenses related to any admission primarily for diagnostic\n",
      "and evaluation purposes only are excluded.\n",
      "b. Any diagnostic expenses which are not related or not\n",
      "incidental to the current diagnosis and treatment are\n",
      "excluded.\n",
      "ii. Rest Cure, rehabilitation and respite care: Code  Excl05 \n",
      "Expenses related to any admission primarily for enforced bed\n",
      "rest and not for receiving treatment. This also includes:\n",
      "a. Custodial care either at home or in a nursing facility for\n",
      "personal care such as help with activities of daily living such\n",
      "as bathing, dressing, moving around either by skilled nurses\n",
      "or assistant or non-skilled persons.\n",
      "b. Any services for people who are terminally ill to address\n",
      "physical, social, emotional and spiritual needs.\n",
      "iii. Obesity/Weight control: Code  Excl06  Expenses related to\n",
      "the surgical treatment of obesity that does not fulfil all the below\n",
      "conditions:\n",
      "a. Surgery to be conducted is upon the advice of the doctor.\n",
      "b. The surgery/procedure conducted should be supported by\n",
      "clinical protocols.\n",
      "c. The member has to be 18 years of age or older and,\n",
      "d. Body Mass Index (BMI)\n",
      "i. Greater than or equal to 40 or,\n",
      "ii. Greater than or equal to 35 in conjunction with any of\n",
      "the following severe co-morbidities following failure of\n",
      "less invasive methods of weight loss:\n",
      "1. Obesity related cardiomyopathy\n",
      "2. Coronary heart disease\n",
      "3. Severe sleep apnoea\n",
      "4. Uncontrolled type2 diabetes\n",
      "iv. Change-of-Gender treatments: Code  Excl07  Expenses\n",
      "related to any treatment, including surgical management, to\n",
      "change characteristics of the body to those of the opposite sex.\n",
      "(Note: The above is a partial listing of the Policy exclusions. Please\n",
      "refer to the Policy clauses for the full listing) Section C.1.d\n",
      "Section C.1.e\n",
      "Section C.1.f\n",
      "Section C.1.g\n",
      "      4                                Waiting\n",
      "Periods                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Pre-existing Diseases will be covered after a waiting period of\n",
      "36 months.\n",
      " 24 months for specific illness and treatments in the first two years\n",
      "and is not applicable in subsequent renewals.\n",
      " 30 days for all illnesses (except accident) in the first year and is\n",
      "not applicable in subsequent renewals.                Section C.1.a\n",
      "Section C.1.b\n",
      "Section C.1.c\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''      0                                               1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        2                                                          3\n",
    "S.\\nNo.                                           Title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Description                                      Policy clause\\nnumber\n",
    "      3 What are\\nthe major\\nexclusions in\\nthe Policy: We will not pay expenses arising from.\\ni. Investigation & Evaluation: Code  Excl04\\na. Expenses related to any admission primarily for diagnostic\\nand evaluation purposes only are excluded.\\nb. Any diagnostic expenses which are not related or not\\nincidental to the current diagnosis and treatment are\\nexcluded.\\nii. Rest Cure, rehabilitation and respite care: Code  Excl05 \\nExpenses related to any admission primarily for enforced bed\\nrest and not for receiving treatment. This also includes:\\na. Custodial care either at home or in a nursing facility for\\npersonal care such as help with activities of daily living such\\nas bathing, dressing, moving around either by skilled nurses\\nor assistant or non-skilled persons.\\nb. Any services for people who are terminally ill to address\\nphysical, social, emotional and spiritual needs.\\niii. Obesity/Weight control: Code  Excl06  Expenses related to\\nthe surgical treatment of obesity that does not fulfil all the below\\nconditions:\\na. Surgery to be conducted is upon the advice of the doctor.\\nb. The surgery/procedure conducted should be supported by\\nclinical protocols.\\nc. The member has to be 18 years of age or older and,\\nd. Body Mass Index (BMI)\\ni. Greater than or equal to 40 or,\\nii. Greater than or equal to 35 in conjunction with any of\\nthe following severe co-morbidities following failure of\\nless invasive methods of weight loss:\\n1. Obesity related cardiomyopathy\\n2. Coronary heart disease\\n3. Severe sleep apnoea\\n4. Uncontrolled type2 diabetes\\niv. Change-of-Gender treatments: Code  Excl07  Expenses\\nrelated to any treatment, including surgical management, to\\nchange characteristics of the body to those of the opposite sex.\\n(Note: The above is a partial listing of the Policy exclusions. Please\\nrefer to the Policy clauses for the full listing) Section C.1.d\\nSection C.1.e\\nSection C.1.f\\nSection C.1.g\n",
    "      4                                Waiting\\nPeriods                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Pre-existing Diseases will be covered after a waiting period of\\n36 months.\\n 24 months for specific illness and treatments in the first two years\\nand is not applicable in subsequent renewals.\\n 30 days for all illnesses (except accident) in the first year and is\\nnot applicable in subsequent renewals.                Section C.1.a\\nSection C.1.b\\nSection C.1.c\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## all docs summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import tiktoken\n",
    "\n",
    "def analyze_documents(documents, processed_documents):\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    metrics = {\n",
    "        'per_pdf': {},\n",
    "        'total': {\n",
    "            'word_count': 0,\n",
    "            'token_count': 0,\n",
    "            'paragraphs': 0,\n",
    "            'page_count': 0,\n",
    "            'tables': 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    processed_pdfs = set()\n",
    "    \n",
    "    for doc in documents:\n",
    "        source = doc.metadata['source']\n",
    "        \n",
    "        if source in processed_pdfs:\n",
    "            continue\n",
    "        processed_pdfs.add(source)\n",
    "        \n",
    "        pdf_doc = fitz.open(source)\n",
    "        page_count = len(pdf_doc)\n",
    "        pdf_doc.close()\n",
    "        \n",
    "        pdf_docs = [d for d in documents if d.metadata['source'] == source]\n",
    "        pdf_processed_docs = [d for d in processed_documents if d.metadata['source'] == source]\n",
    "        \n",
    "        pdf_metrics = {\n",
    "            'word_count': 0,\n",
    "            'token_count': 0,\n",
    "            'paragraphs': 0,\n",
    "            'page_count': page_count,\n",
    "            'tables': 0\n",
    "        }\n",
    "        \n",
    "        for doc in pdf_docs:\n",
    "            if doc.metadata.get('type') == 'table':\n",
    "                pdf_metrics['tables'] += 1\n",
    "                metrics['total']['tables'] += 1\n",
    "            \n",
    "            if doc.page_content:\n",
    "                words = doc.page_content.split()\n",
    "                tokens = tokenizer.encode(doc.page_content)\n",
    "                paragraphs = doc.page_content.split('\\n\\n')\n",
    "                \n",
    "                pdf_metrics['word_count'] += len(words)\n",
    "                pdf_metrics['token_count'] += len(tokens)\n",
    "                pdf_metrics['paragraphs'] += len(paragraphs)\n",
    "        \n",
    "        metrics['total']['word_count'] += pdf_metrics['word_count']\n",
    "        metrics['total']['token_count'] += pdf_metrics['token_count']\n",
    "        metrics['total']['paragraphs'] += pdf_metrics['paragraphs']\n",
    "        metrics['total']['page_count'] += page_count\n",
    "        \n",
    "        metrics['per_pdf'][source] = pdf_metrics\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_document_analysis(metrics):\n",
    "    print(\"\\n--- DOCUMENT ANALYSIS ---\")\n",
    "    \n",
    "    print(\"\\nPer PDF Metrics:\")\n",
    "    for pdf, pdf_metrics in metrics['per_pdf'].items():\n",
    "        print(f\"\\nPDF: {pdf}\")\n",
    "        print(f\"  Word Count: {pdf_metrics['word_count']}\")\n",
    "        print(f\"  Token Count: {pdf_metrics['token_count']}\")\n",
    "        print(f\"  Paragraphs: {pdf_metrics['paragraphs']}\")\n",
    "        print(f\"  Page Count: {pdf_metrics['page_count']}\")\n",
    "        print(f\"  Tables: {pdf_metrics['tables']}\")\n",
    "    \n",
    "    print(\"\\nTotal Metrics:\")\n",
    "    total = metrics['total']\n",
    "    print(f\"  Total Word Count: {total['word_count']}\")\n",
    "    print(f\"  Total Token Count: {total['token_count']}\")\n",
    "    print(f\"  Total Paragraphs: {total['paragraphs']}\")\n",
    "    print(f\"  Total Page Count: {total['page_count']}\")\n",
    "    print(f\"  Total Tables: {total['tables']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
