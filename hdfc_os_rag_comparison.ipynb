{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from pydantic import BaseModel\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_key = \"AIzaSyAHPpqUignpGcTI1ZfmXfcFcxlpKDtDSrQ\"\n",
    "api_key = \"AIzaSyCPfIdMffhoR2nxre5pmCFuYmvEI6G7oyY\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name= \"gemini-1.5-flash\"\n",
    "llm = ChatGoogleGenerativeAI(model=model_name,temperature=0.0, google_api_key=api_key, max_tokens=None)\n",
    "# !pip install langchain-google-genai chromadb pypdf pillow unstructured[local-inference] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Txt RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\sadik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sadik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\sadik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from pydantic import BaseModel\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'hdfc_os_txt'\n",
    "glob_pattern = \"**/*.txt\"\n",
    "loader = DirectoryLoader(path=path, glob=glob_pattern)\n",
    "pages_txt = loader.load()\n",
    "len(pages_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vectorstore_txt = Chroma.from_documents(\n",
    "            documents=pages_txt, \n",
    "            embedding=embeddings,\n",
    "        collection_name=\"txt_documents\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "print(vectorstore_txt._collection.count()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_template = '''\n",
    "Imagine you are an expert insurance advisor with deep knowledge of various insurance policies, including health, auto, life, and property insurance. Your goal is to provide clear, accurate, and user-friendly answers to customer queries based on the most relevant documents retrieved.\n",
    "\n",
    "For each query:\n",
    "\n",
    "Carefully analyze the retrieved documents to extract the most relevant information.\n",
    "Summarize complex policy details in a way that is easy to understand.\n",
    "Address any potential concerns the user might have, providing additional context if necessary.\n",
    "If the retrieved information is insufficient, acknowledge the limitation and suggest general best practices.\n",
    "Ensure your responses are professional, concise, and informative while maintaining a helpful and friendly tone.\n",
    "Make sure you give correct answer which is relevent to the context\n",
    "\n",
    "Use this Context to answer the Question: \n",
    "{context}\n",
    "    \n",
    "Question: {question}\n",
    "    \n",
    "Detailed Answer:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def create_rag_chain(vectorstore):\n",
    "    # Retriever\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "    \n",
    "    # Prompt Template\n",
    "    prompt_template = ChatPromptTemplate.from_template(rag_template)\n",
    "    \n",
    "    # RAG Chain\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt_template\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return rag_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_txt = create_rag_chain(vectorstore_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The HDFC Optima Secure policy is a health insurance plan offering a range of coverages.  The base coverage includes hospitalization expenses (medical expenses for minimum 24-hour stays, room rent, road ambulance), home healthcare, domiciliary hospitalization, AYUSH treatment, pre-hospitalization (60 days prior) and post-hospitalization (180 days after) medical expenses, and organ donor expenses.  Note that the coverage is up to the sum insured and limits specified in your policy schedule, subject to policy terms and conditions.\n",
      "\n",
      "Several optional covers are available for an additional premium, including emergency air ambulance, daily cash for shared rooms, various benefit packages (Protect, Plus, Secure, Automatic Restore), aggregate deductible, e-opinion for critical illness, global health cover (emergency only or emergency and planned treatments), and Overseas Travel Secure.  These optional covers have sub-limits detailed in your policy schedule.  The availability of these optional covers depends on whether they are specified as being in force in your policy schedule.\n",
      "\n",
      "The Overseas Travel Secure optional cover, available only with Optima Secure Global or Optima Secure Global Plus plans, reimburses travel and accommodation expenses for the insured person and one accompanying person if hospitalization of at least 5 consecutive days is required overseas and a claim under the Global Health Cover (emergency or emergency & planned) is approved.  Accommodation is covered up to Rs. 15,000 per day for a maximum of 30 days per policy year.  Only basic economy airfare is covered.\n",
      "\n",
      "A preventive health checkup is offered upon each continuous policy renewal, up to the amount specified in your policy schedule.  This benefit does not carry over if unclaimed and is not provided if the policy isn't renewed.\n",
      "\n",
      "**Important Note:**  The provided documents are excerpts and don't contain the full policy details, including the specific sum insured, sub-limits for optional covers, and complete terms and conditions.  For complete information and to understand your specific coverage, please refer to your full policy document.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'tell me about hdfc optima secure policy'\n",
    "response = rag_chain_txt.invoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"docs\\HDFC\\HDFC-optima_secure\\HDFC-optima_secure_policywordings.pdf\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DOCUMENT PROCESSING SUMMARY ---\n",
      "Total original documents: 65\n",
      "Total processed chunks: 65\n",
      "Total text chunks: 34\n",
      "Total table chunks: 31\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def extract_tables_and_text(pdf_path):\n",
    "    documents = []\n",
    "    \n",
    "    try:\n",
    "        # Open the PDF\n",
    "        doc = fitz.open(pdf_path)\n",
    "        \n",
    "        for page_num, page in enumerate(doc):\n",
    "            # Extract text\n",
    "            text_content = page.get_text()\n",
    "            \n",
    "            # Extract tables\n",
    "            tables = page.find_tables()\n",
    "            \n",
    "            # Add text as a document\n",
    "            if text_content.strip():\n",
    "                documents.append(Document(\n",
    "                    page_content=text_content,\n",
    "                    metadata={\n",
    "                        'source': pdf_path,\n",
    "                        'page': page_num,\n",
    "                        'type': 'text'\n",
    "                    }\n",
    "                ))\n",
    "            \n",
    "            # Add tables as separate documents\n",
    "            for table_index, table in enumerate(tables):\n",
    "                # Convert table to markdown\n",
    "                table_markdown = table.to_markdown()\n",
    "                \n",
    "                if table_markdown.strip():\n",
    "                    documents.append(Document(\n",
    "                        page_content=table_markdown,\n",
    "                        metadata={\n",
    "                            'source': pdf_path,\n",
    "                            'page': page_num,\n",
    "                            'table_index': table_index,\n",
    "                            'type': 'table'\n",
    "                        }\n",
    "                    ))\n",
    "        \n",
    "        doc.close()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path}: {e}\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "def split_documents(documents):\n",
    "    final_chunks = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        if doc.metadata['type'] == 'table':\n",
    "            # Keep entire table as a chunk\n",
    "            final_chunks.append(Document(\n",
    "                page_content=doc.page_content,\n",
    "                metadata={\n",
    "                    **doc.metadata,\n",
    "                    'content_type': 'table'\n",
    "                }\n",
    "            ))\n",
    "        elif doc.metadata['type'] == 'text':\n",
    "            # Keep text as is without splitting\n",
    "            final_chunks.append(Document(\n",
    "                page_content=doc.page_content,\n",
    "                metadata={\n",
    "                    **doc.metadata,\n",
    "                    'content_type': 'text'\n",
    "                }\n",
    "            ))\n",
    "    return final_chunks\n",
    "\n",
    "def main(pdf_path):\n",
    "    documents = extract_tables_and_text(pdf_path)\n",
    "    processed_documents = split_documents(documents)\n",
    "    \n",
    "    print(\"\\n--- DOCUMENT PROCESSING SUMMARY ---\")\n",
    "    print(f\"Total original documents: {len(documents)}\")\n",
    "    print(f\"Total processed chunks: {len(processed_documents)}\")\n",
    "    \n",
    "    # Count and print types of chunks\n",
    "    text_chunks = [doc for doc in processed_documents if doc.metadata.get('content_type') == 'text']\n",
    "    table_chunks = [doc for doc in processed_documents if doc.metadata.get('content_type') == 'table']\n",
    "    \n",
    "    print(f\"Total text chunks: {len(text_chunks)}\")\n",
    "    print(f\"Total table chunks: {len(table_chunks)}\")\n",
    "    \n",
    "    return documents, processed_documents, text_chunks, table_chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    documents_pdf, processed_documents_pdf, text_chunks_pdf, table_chunks_pdf = main(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_pdf = Chroma.from_documents(\n",
    "        documents=processed_documents_pdf,\n",
    "        embedding=embeddings,\n",
    "        collection_name=\"pdf_documents\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n"
     ]
    }
   ],
   "source": [
    "print(vectorstore_pdf._collection.count()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_pdf = create_rag_chain(vectorstore_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The HDFC ERGO Optima Secure policy is a health insurance plan offering a range of coverages.  The base coverage (Section B) includes hospitalization expenses (covering medical expenses for minimum 24-hour stays, room rent, road ambulance, inpatient dental and plastic surgery due to injury, and daycare procedures), home healthcare, domiciliary hospitalization, AYUSH treatment, pre-hospitalization expenses (60 days prior), post-hospitalization expenses (180 days after), and organ donor expenses.  A cumulative bonus may also apply, but this is only for the Optima Suraksha plan (details not provided in these documents).\n",
      "\n",
      "Optional covers (Section B-2) are available with sub-limits specified in the policy schedule. These include emergency air ambulance, daily cash for shared rooms, Protect Benefit, Plus Benefit, Secure Benefit, Automatic Restore Benefit, aggregate deductible, e-opinion for critical illness, global health cover (emergency only or emergency and planned treatments), and overseas travel secure.  These optional covers are only applicable if specifically included in your policy schedule.\n",
      "\n",
      "The policy operates on an indemnity basis for most claims, meaning you are reimbursed for expenses incurred up to the sum insured and specified limits.  However, daily cash for a shared room is a benefit-based claim.  An aggregate deductible (if opted for) applies to claims throughout the policy year.\n",
      "\n",
      "The policy is renewable for life.  Premium payment options include yearly, half-yearly, quarterly, and monthly installments.  Grace periods vary depending on the payment frequency (30 days for yearly, half-yearly, and quarterly; 15 days for monthly).  Renewal benefits may include a cumulative bonus, Plus Benefit, and preventive health check-ups (specific details on these benefits are not fully provided in the given documents).\n",
      "\n",
      "**Important Note:**  The provided documents are excerpts and don't contain the full policy details, including specific sum insured amounts, sub-limits for optional covers, and complete descriptions of the renewal benefits.  For complete information and to understand the policy's terms and conditions fully, please refer to your complete policy document.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'tell me about hdfc optima secure policy'\n",
    "response = rag_chain_pdf.invoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MD RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DOCUMENT PROCESSING SUMMARY ---\n",
      "Total original documents: 62\n",
      "Total processed chunks: 62\n",
      "Total text chunks: 31\n",
      "Total table chunks: 31\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def extract_sections_and_tables(md_path):\n",
    "    documents = []\n",
    "    \n",
    "    try:\n",
    "        # Read the entire Markdown file\n",
    "        with open(md_path, 'r', encoding='utf-8') as file:\n",
    "            md_content = file.read()\n",
    "        \n",
    "        # Split content into sections based on headers\n",
    "        sections = re.split(r'(^#+\\s+.*$)', md_content, flags=re.MULTILINE)\n",
    "        \n",
    "        current_header = None\n",
    "        current_section_content = []\n",
    "        \n",
    "        for section in sections:\n",
    "            # Check if this is a header\n",
    "            header_match = re.match(r'^(#+)\\s+(.*)$', section)\n",
    "            if header_match:\n",
    "                # If we had a previous section, add it to documents\n",
    "                if current_header and current_section_content:\n",
    "                    documents.append(Document(\n",
    "                        page_content=''.join(current_section_content).strip(),\n",
    "                        metadata={\n",
    "                            'source': md_path,\n",
    "                            'header': current_header,\n",
    "                            'type': 'text'\n",
    "                        }\n",
    "                    ))\n",
    "                \n",
    "                # Reset for new section\n",
    "                current_header = section.strip()\n",
    "                current_section_content = [section]\n",
    "            else:\n",
    "                # Accumulate section content\n",
    "                if current_header:\n",
    "                    current_section_content.append(section)\n",
    "        \n",
    "        # Add the last section\n",
    "        if current_header and current_section_content:\n",
    "            documents.append(Document(\n",
    "                page_content=''.join(current_section_content).strip(),\n",
    "                metadata={\n",
    "                    'source': md_path,\n",
    "                    'header': current_header,\n",
    "                    'type': 'text'\n",
    "                }\n",
    "            ))\n",
    "        \n",
    "        # Extract tables\n",
    "        table_pattern = r'(\\|.*\\|\\n)(\\|[-:| ]+\\|\\n)(\\|.*\\|\\n)+'\n",
    "        tables = re.findall(table_pattern, md_content, re.MULTILINE)\n",
    "        \n",
    "        for table_index, table_match in enumerate(tables):\n",
    "            table_markdown = ''.join(table_match)\n",
    "            documents.append(Document(\n",
    "                page_content=table_markdown.strip(),\n",
    "                metadata={\n",
    "                    'source': md_path,\n",
    "                    'table_index': table_index,\n",
    "                    'type': 'table'\n",
    "                }\n",
    "            ))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {md_path}: {e}\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "def split_documents(documents):\n",
    "    final_chunks = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        if doc.metadata['type'] == 'table':\n",
    "            # Keep entire table as a chunk\n",
    "            final_chunks.append(Document(\n",
    "                page_content=doc.page_content,\n",
    "                metadata={\n",
    "                    **doc.metadata,\n",
    "                    'content_type': 'table'\n",
    "                }\n",
    "            ))\n",
    "        elif doc.metadata['type'] == 'text':\n",
    "            # Keep text as is without splitting\n",
    "            final_chunks.append(Document(\n",
    "                page_content=doc.page_content,\n",
    "                metadata={\n",
    "                    **doc.metadata,\n",
    "                    'content_type': 'text'\n",
    "                }\n",
    "            ))\n",
    "    return final_chunks\n",
    "\n",
    "def main(md_path):\n",
    "    documents = extract_sections_and_tables(md_path)\n",
    "    processed_documents = split_documents(documents)\n",
    "    \n",
    "    print(\"\\n--- DOCUMENT PROCESSING SUMMARY ---\")\n",
    "    print(f\"Total original documents: {len(documents)}\")\n",
    "    print(f\"Total processed chunks: {len(processed_documents)}\")\n",
    "    \n",
    "    # Count and print types of chunks\n",
    "    text_chunks = [doc for doc in processed_documents if doc.metadata.get('content_type') == 'text']\n",
    "    table_chunks = [doc for doc in processed_documents if doc.metadata.get('content_type') == 'table']\n",
    "    \n",
    "    print(f\"Total text chunks: {len(text_chunks)}\")\n",
    "    print(f\"Total table chunks: {len(table_chunks)}\")\n",
    "    \n",
    "    return documents, processed_documents, text_chunks, table_chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    documents_md, processed_documents_md, text_chunks_md, table_chunks_md = main(\"output.md\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_md = Chroma.from_documents(\n",
    "        documents=processed_documents_md,\n",
    "        embedding=embeddings,\n",
    "        collection_name=\"md_documents\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_md = create_rag_chain(vectorstore_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124\n"
     ]
    }
   ],
   "source": [
    "print(vectorstore_md._collection.count()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The HDFC ERGO Optima Secure policy offers a range of health insurance benefits, with several options for coverage and premium payment.  Based on the provided documents, here's a summary:\n",
      "\n",
      "**Coverage:**  Optima Secure provides indemnity-based coverage for various services including hospitalization, road ambulance, home healthcare, domiciliary treatment, AYUSH treatment, organ donor expenses, emergency air ambulance, and preventive health check-ups.  It also includes benefits like \"Protect Benefit,\" \"Secure Benefit,\" and \"Plus Benefit\" (details on these specific benefits are not provided in the document).  Depending on the plan chosen (Optima Secure, Optima Secure Global, or Optima Secure Global Plus),  coverage extends to emergency and/or planned treatments globally or is limited to India.  Overseas Travel Secure is an optional add-on.  A daily cash benefit for shared rooms is also available under a benefit basis.\n",
      "\n",
      "**Payment and Renewal:** Premiums can be paid annually, semi-annually, quarterly, or monthly. The policy is renewable for life.  Grace periods for premium payments vary depending on the payment frequency (30 days for yearly/half-yearly/quarterly, 15 days for monthly).\n",
      "\n",
      "**Loss Sharing:** An aggregate deductible (amount you pay before the insurance kicks in) applies to claims during the policy year.  The specific deductible amount varies depending on the plan selected (25k/50k/100k/200k/300k options are mentioned).\n",
      "\n",
      "**Add-on Covers:** The policy allows for optional add-ons, including critical illness coverage, hospital cash benefits, personal accident rider, and unlimited restore benefit.  Details on these add-ons require reviewing the prospectus and policy wording documents.\n",
      "\n",
      "**Limitations:** The provided documents lack detailed explanations of several benefits (\"Protect Benefit,\" \"Secure Benefit,\" \"Plus Benefit\") and don't specify the exact coverage amounts or limits for many of the included services.  To get a complete understanding of the policy's terms and conditions, including specific coverage details and exclusions, you should refer to the full policy document and/or contact HDFC ERGO directly.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'tell me about hdfc optima secure policy'\n",
    "response = rag_chain_md.invoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What is the name of the insurance product discussed in the document?\"\n",
    "# query = \"What is the waiting period for pre-existing diseases under this policy?\"\n",
    "# query = \"Which modes of premium payment are available for this policy?\"\n",
    "query = \"What are the key exclusions under this policy?\"\n",
    "# query = \"What are the optional covers available under the policy?\"\n",
    "# query = \"How does the cumulative bonus work in this policy?\"\n",
    "# query = \"What conditions must be met for surgical obesity treatment to be covered under this policy?\"\n",
    "# query = \"What are the eligibility conditions for using the 'Waiver of Aggregate Deductible' option?\"\n",
    "# query = \"What is the difference between Home Health Care and Domiciliary Hospitalization in this policy?\"\n",
    "# query = \"How does the Automatic Restore Benefit work in the policy?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = rag_chain_txt.invoke(query)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = rag_chain_pdf.invoke(query)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = rag_chain_md.invoke(query)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the context documents\n",
    "retriever_txt = vectorstore_txt.as_retriever(search_kwargs={\"k\": 4})\n",
    "retriever_pdf = vectorstore_pdf.as_retriever(search_kwargs={\"k\": 4})\n",
    "retriever_md = vectorstore_md.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "context_docs = retriever.invoke(\"Your query\")\n",
    "\n",
    "# Combine all context documents into a single string\n",
    "full_context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in context_docs])\n",
    "\n",
    "print(\"Full Context:\")\n",
    "print(full_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
